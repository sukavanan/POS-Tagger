{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d94f0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fbc2909e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict = {}            \n",
    "#dictionary for the vocabulary and its frequency, key - word, value - frequency\n",
    "pos_freq_dict = {}         \n",
    "#dictionary for the pos tags and its frequence, key - pos tag, value - frequency\n",
    "transition = {}            \n",
    "#dictionary to maintain t(s'|s), key - (s, s'), value - t(s'|s)\n",
    "emission = {}              \n",
    "#dictionary to maintain e(s|x), key - (s, x), value - e(s|x)\n",
    "pos_idx_dict = {}          \n",
    "#dictionary to maintain the index to pos tag matching, key - index, value - pos tag\n",
    "pos_idx_dict_inv = {}      \n",
    "#inverse of above dictionary, key - pos tag, value - index\n",
    "vocab_idx_dict = {}        \n",
    "#dictionary to maintain the index to word matching, key - index, value - word\n",
    "vocab_idx_dict_inv = {}    \n",
    "#inverse of above dictionary, key - word, value - index\n",
    "sentences = 0\n",
    "threshold = 1\n",
    "unknown_count = 0\n",
    "\n",
    "#list of numbers in words, used to assign the <num> tag\n",
    "numbers = ['one','two','three','four','five', 'six','seven','eight','nine', 'ten', 'eleven', 'twelve', 'thirteen', 'fourteen', 'fifteen', 'sixteen', 'seventeen', 'eighteen', 'nineteen', 'twenty', 'thirty', 'forty', 'fifty', 'sixty', 'seventy', 'eighty', 'ninety', 'zero', 'hundred', 'thousand', 'million', 'billion', 'trillion', 'quadrillion', 'quintillion', 'sextillion', 'septillion', 'octillion', 'nonillion', 'decillion']\n",
    "\n",
    "#file paths\n",
    "train_path = \"data/train\"\n",
    "dev_path = \"data/dev\"\n",
    "test_path = \"data/test\"\n",
    "vocab_path = \"data/vocab.txt\"\n",
    "hmm_path = \"data/hmm.json\"\n",
    "greedy_path = \"outputs/greedy.out\"\n",
    "viterbi_path = \"outputs/viterbi.out\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3d0dd079",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to identify if a given word is a number\n",
    "def is_number(s):\n",
    "    try:\n",
    "        if \",\" in s:\n",
    "            s = s.replace(\",\", \"\")\n",
    "        if \":\" in s:\n",
    "            s = s.replace(\":\", \"\")\n",
    "        if \"\\/\" in s:\n",
    "            s = s.replace(\"\\/\", \"\")\n",
    "        if s.lower() in numbers:\n",
    "            return True\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "#function to identify if a given word is a compound word\n",
    "def is_compound(s):\n",
    "    if \"-\" in s:\n",
    "        s = s.replace(\"-\", \"\")\n",
    "        if s.isalnum():\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "13331de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the vocabulary\n",
    "with open(train_path, \"r\") as file_obj:\n",
    "    lines = file_obj.readlines()\n",
    "line_list = [line.rstrip().split(\"\\t\") for line in lines]\n",
    "line_list = [x for x in line_list if x != ['']]\n",
    "\n",
    "for index, word, pos in line_list:\n",
    "    if index == '1':\n",
    "        sentences += 1\n",
    "    if is_number(word):\n",
    "        word = \"<num>\"\n",
    "    elif is_compound(word):\n",
    "        word = \"<cmp>\"\n",
    "    if word in vocab_dict:\n",
    "        vocab_dict[word] += 1\n",
    "    else:\n",
    "        vocab_dict[word] = 1\n",
    "    if pos in pos_freq_dict:\n",
    "        pos_freq_dict[pos] += 1\n",
    "    else:\n",
    "        pos_freq_dict[pos] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b93c9f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pos_tag = max(pos_freq_dict, key = pos_freq_dict.get)\n",
    "\n",
    "#replacing rare words with <unk>\n",
    "for word, freq in list(vocab_dict.items()):\n",
    "    if freq <= threshold:\n",
    "        unknown_count += freq\n",
    "        del vocab_dict[word]\n",
    "\n",
    "vocab_dict[\"<unk>\"] = unknown_count\n",
    "vocab_dict = dict(sorted(vocab_dict.items(), key = lambda item: item[1], reverse = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d5ddb6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the vocab.txt file\n",
    "index = 1\n",
    "with open(vocab_path, \"w\") as fp:\n",
    "    fp.write(\"<unk>\\t{}\\t{}\\n\".format(index, str(unknown_count)))\n",
    "    for word, freq in vocab_dict.items():\n",
    "        index += 1\n",
    "        if word != \"<unk>\":\n",
    "            fp.write(\"{}\\t{}\\t{}\\n\".format(word, index, freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1d7e6027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold for rare words: <= 1\n",
      "Size of vocabulary:  20067\n",
      "Total no.of sentences:  38218\n",
      "Total number of occurunces of <unk>:  13887\n"
     ]
    }
   ],
   "source": [
    "print(\"Threshold for rare words: <=\", threshold)\n",
    "print(\"Size of vocabulary: \", len(vocab_dict))\n",
    "print(\"Total no.of sentences: \", sentences)\n",
    "print(\"Total number of occurunces of <unk>: \",vocab_dict[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "22940d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the transition and emission dictionaries\n",
    "for i in range(len(line_list) - 1):\n",
    "    pos = line_list[i][2]\n",
    "    pos_ = line_list[i+1][2]\n",
    "    word = line_list[i][1]\n",
    "    \n",
    "    if is_number(word):\n",
    "        word = \"<num>\"\n",
    "    elif is_compound(word):\n",
    "        word = \"<cmp>\"\n",
    "    \n",
    "    if word not in vocab_dict:\n",
    "        word = \"<unk>\"\n",
    "    \n",
    "    denom = pos_freq_dict[pos]\n",
    "    \n",
    "    if line_list[i][0] == '1':\n",
    "        if (\"<s>\", pos) in transition:\n",
    "            transition[(\"<s>\", pos)] += (1 / sentences)\n",
    "        else:\n",
    "            transition[(\"<s>\", pos)] = (1 / sentences)\n",
    "    else:\n",
    "        if (pos, pos_) in transition:\n",
    "            transition[(pos, pos_)] += (1 / denom)\n",
    "        else:\n",
    "            transition[(pos, pos_)] = (1 / denom)\n",
    "    \n",
    "    if (pos, word) in emission:\n",
    "        emission[(pos, word)] += (1 / denom)\n",
    "    else:\n",
    "        emission[(pos, word)] = (1 / denom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6d1d4944",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping 0 probabilities\n",
    "for key, val in list(transition.items()):\n",
    "    if val == 0:\n",
    "        del transition[key]\n",
    "\n",
    "for key, val in list(emission.items()):\n",
    "    if val == 0:\n",
    "        del emission[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "55c8acfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the keys to string so that it could be stored as json file\n",
    "str_transition = {key[0] + \",\" + key[1]: val for key, val in transition.items()}\n",
    "str_emission = {key[0] + \",\" + key[1]: val for key, val in emission.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f38a2f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no.of transmission probabilities:  1410\n",
      "Total no.of emission probabilities:  26854\n"
     ]
    }
   ],
   "source": [
    "print(\"Total no.of transmission probabilities: \",len(transition))\n",
    "print(\"Total no.of emission probabilities: \",len(emission))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c08f9778",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_dict = {\"transition\":str_transition, \"emission\":str_emission}\n",
    "with open(hmm_path, \"w\") as fp:\n",
    "    json.dump(json_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "48844425",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to predict pos tag based on the greedy decoding of hmm algorithm\n",
    "def greedy_decoding(pos, word):\n",
    "    temp_trans = {key[1]: val for key, val in transition.items() if key[0] == pos}\n",
    "    temp_emiss = {key[0]: val for key, val in emission.items() if key[1] == word}\n",
    "    max_val = 0.0\n",
    "    pos_tag = max_pos_tag\n",
    "    flag = 0\n",
    "    \n",
    "    trans_set = set(temp_trans)\n",
    "    emiss_set = set(temp_emiss)\n",
    "    for key in trans_set.intersection(emiss_set):\n",
    "        flag = 1\n",
    "        res = temp_trans[key] * temp_emiss[key]\n",
    "        if res > max_val:\n",
    "            max_val = res\n",
    "            pos_tag = key\n",
    "\n",
    "#flag variable used to check if there is a common pos tag between the transition set and the emission set\n",
    "#if no common pos tag found, and emission set is not empty\n",
    "#then the max valued pos tag from the emission set is returned\n",
    "#else, max values pos tag from transition set is returned\n",
    "#if both are empty the most common pos tag in the whole dataset is returned\n",
    "    if flag == 0:\n",
    "        if len(temp_emiss) != 0:\n",
    "            return max(temp_emiss, key = temp_emiss.get)\n",
    "        elif len(temp_trans) != 0:\n",
    "            return max(temp_trans, key = temp_trans.get)\n",
    "        else:\n",
    "            return pos_tag\n",
    "        \n",
    "    return pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2260b01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that calls the greedy_decoding, unknown, number and compound words are replaced with their respective tags\n",
    "def predict(dataset):\n",
    "    correct = 0\n",
    "    for i in range(len(dataset)):\n",
    "        word = dataset[i][1]\n",
    "        pos = dataset[i][2]\n",
    "        if is_number(word):\n",
    "            word = \"<num>\"\n",
    "        elif is_compound(word):\n",
    "            word = \"<cmp>\"\n",
    "        if word not in vocab_dict:\n",
    "            word = \"<unk>\"\n",
    "        if dataset[i][0] == '1':\n",
    "            pred_tag = greedy_decoding(\"<s>\", word)\n",
    "        else:\n",
    "            prev_pos = dataset[i-1][2]\n",
    "            pred_tag = greedy_decoding(prev_pos, word)\n",
    "        if pred_tag == pos:\n",
    "            correct += 1\n",
    "        elif pred_tag == -1:\n",
    "            print(i, pos)\n",
    "            break\n",
    "    return (correct / len(dataset)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "843b514c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to create the greedy.out file for the test dataset\n",
    "def test(dataset):\n",
    "    with open(greedy_path, \"w\") as fp:\n",
    "        pos = \"\"\n",
    "        for i in range(len(dataset)):\n",
    "            index = dataset[i][0]\n",
    "            word = dataset[i][1]\n",
    "            if index == '1':\n",
    "                pos = \"<s>\"\n",
    "            if is_number(word):\n",
    "                word = \"<num>\"\n",
    "            elif is_compound(word):\n",
    "                word = \"<cmp>\"\n",
    "            if word not in vocab_dict:\n",
    "                word = \"<unk>\"\n",
    "            pred_tag = greedy_decoding(pos, word)\n",
    "            pos = pred_tag\n",
    "            fp.write(\"{}\\t{}\\t{}\\n\".format(index, dataset[i][1], pred_tag))\n",
    "            if i != len(dataset) - 1 and dataset[i+1][0] == '1':\n",
    "                fp.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ffabf7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the index-pos tag (and its inverse) and the index-word (and its inverse) dictionaries\n",
    "idx = 0\n",
    "for key, val in pos_freq_dict.items():\n",
    "    pos_idx_dict[idx] = key\n",
    "    pos_idx_dict_inv[key] = idx\n",
    "    idx += 1\n",
    "\n",
    "idx = 0\n",
    "for key, val in vocab_dict.items():\n",
    "    vocab_idx_dict[idx] = key\n",
    "    vocab_idx_dict_inv[key] = idx\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dbeb653d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the prior, transmission and emission matrices for the viterbi decoding algorithm\n",
    "prior = np.zeros((len(pos_idx_dict),), dtype = float)\n",
    "for i in range(len(pos_idx_dict)):\n",
    "    if (\"<s>\", pos_idx_dict[i]) in transition:\n",
    "        prior[i] = transition[(\"<s>\", pos_idx_dict[i])]\n",
    "    else:\n",
    "        prior[i] = 0\n",
    "\n",
    "tm = np.zeros((len(pos_idx_dict), len(pos_idx_dict)), dtype = float)\n",
    "for i in range(len(pos_idx_dict)):\n",
    "    for j in range(len(pos_idx_dict)):\n",
    "        if (pos_idx_dict[i], pos_idx_dict[j]) in transition:\n",
    "            tm[i][j] = transition[(pos_idx_dict[i], pos_idx_dict[j])]\n",
    "        else:\n",
    "            tm[i][j] = 0\n",
    "\n",
    "em = np.zeros((len(pos_idx_dict), len(vocab_idx_dict)), dtype = float)\n",
    "for i in range(len(pos_idx_dict)):\n",
    "    for j in range(len(vocab_idx_dict)):\n",
    "        if (pos_idx_dict[i], vocab_idx_dict[j]) in emission:\n",
    "            em[i][j] = emission[(pos_idx_dict[i], vocab_idx_dict[j])]\n",
    "        else:\n",
    "            em[i][j] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2aa956e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to predict pos tag based on the viterbi decoding for hmm algorithm\n",
    "def viterbi_decoding(sent_data):\n",
    "    samples = len(sent_data)\n",
    "    pos_tags = len(pos_idx_dict)\n",
    "    #the dynamic matrix\n",
    "    viterbi = np.zeros((pos_tags, samples), dtype = float)\n",
    "    #matrix to hold the previous max tag for a tag, word pair\n",
    "    path = np.zeros((pos_tags, samples), dtype = int)\n",
    "    #array that gives the indices of the pos tags that maximizes the probability\n",
    "    result_path = np.zeros(samples, dtype = int)\n",
    "    #scale factor used to prevent underflow of probabilities\n",
    "    scale = np.zeros(samples, dtype = float)\n",
    "    \n",
    "    word = sent_data[0]\n",
    "    viterbi[:, 0] = prior.T * em[:, vocab_idx_dict_inv[word]]\n",
    "    \n",
    "    denom = np.sum(viterbi[:, 0])\n",
    "    if denom != 0:\n",
    "        scale[0] = 1.0 / denom\n",
    "        viterbi[:, 0] = scale[0] * viterbi[:, 0]\n",
    "    \n",
    "    #the loops, to calculate [i,j]th value of the viterbi matrix\n",
    "    for i in range(1, samples):\n",
    "        word = sent_data[i]\n",
    "        for j in range(pos_tags):\n",
    "            temp_trans = viterbi[:, i-1] * tm[:, j]\n",
    "            #below function returns the index of max value and the max value \n",
    "            path[j, i], viterbi[j, i] = max(enumerate(temp_trans), key = operator.itemgetter(1))\n",
    "            viterbi[j, i] = viterbi[j, i] * em[j, vocab_idx_dict_inv[word]]\n",
    "        \n",
    "        denom = np.sum(viterbi[:, i])\n",
    "        if denom != 0:\n",
    "            scale[i] = 1 / denom\n",
    "            viterbi[:, i] = scale[i] * viterbi[:, i]\n",
    "    \n",
    "    #backtracking through the path matrix to find the highest probability resulting pos tags\n",
    "    result_path[samples - 1] = viterbi[:, samples - 1].argmax()\n",
    "    for i in range(samples - 1, 0, -1):\n",
    "        result_path[i - 1] = path[result_path[i], i]\n",
    "    \n",
    "    return result_path\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bcebc030",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that calls the viterbi_decoding function, replaces unknown, number and compound words with their respective tags\n",
    "def predict_viterbi(dataset):\n",
    "    correct = 0\n",
    "    sent_data = []\n",
    "    sent_pos = []\n",
    "    for i in range(len(dataset)):\n",
    "        index = dataset[i][0]\n",
    "        word = dataset[i][1]\n",
    "        pos = dataset[i][2]\n",
    "\n",
    "        if is_number(word):\n",
    "            word = \"<num>\"\n",
    "        elif is_compound(word):\n",
    "            word = \"<cmp>\"\n",
    "        if word not in vocab_dict:\n",
    "            word = \"<unk>\"\n",
    "        \n",
    "        #dividing the whole dataset into sentences and sending it to the viterbi_decoding function\n",
    "        sent_data.append(word)\n",
    "        sent_pos.append(pos)        \n",
    "        if i == len(dataset) - 1:\n",
    "            pred_tag = []\n",
    "            path = viterbi_decoding(sent_data)\n",
    "            pred_tag = [pos_idx_dict[path[i]] for i in range(len(path))]\n",
    "            correct += (np.array(pred_tag) == np.array(sent_pos)).sum()\n",
    "        \n",
    "        elif dataset[i+1][0] == '1':\n",
    "            pred_tag = []\n",
    "            path = viterbi_decoding(sent_data)\n",
    "            pred_tag = [pos_idx_dict[path[i]] for i in range(len(path))]\n",
    "            correct += (np.array(pred_tag) == np.array(sent_pos)).sum()\n",
    "            sent_data = []\n",
    "            sent_pos = []\n",
    "    return (correct / len(dataset)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "34915cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to predict the pos tags for the test dataset\n",
    "def test_viterbi(dataset):    \n",
    "    with open(viterbi_path, \"w\") as fp:\n",
    "        sent_data = []\n",
    "        orig_word = []\n",
    "        for i in range(len(dataset)):\n",
    "            index = dataset[i][0]\n",
    "            word = dataset[i][1]\n",
    "            orig_word.append(word)\n",
    "            \n",
    "            if is_number(word):\n",
    "                word = \"<num>\"\n",
    "            elif is_compound(word):\n",
    "                word = \"<cmp>\"\n",
    "            if word not in vocab_dict:\n",
    "                word = \"<unk>\"\n",
    "\n",
    "            sent_data.append(word)        \n",
    "            if i == len(dataset) - 1:\n",
    "                pred_tag = []\n",
    "                path = viterbi_decoding(sent_data)\n",
    "                pred_tag = [pos_idx_dict[path[i]] for i in range(len(path))]\n",
    "                for j in range(len(sent_data)):\n",
    "                    fp.write(\"{}\\t{}\\t{}\\n\".format(j+1, orig_word[j], pred_tag[j]))\n",
    "            \n",
    "            elif dataset[i+1][0] == '1':\n",
    "                pred_tag = []\n",
    "                path = viterbi_decoding(sent_data)\n",
    "                pred_tag = [pos_idx_dict[path[i]] for i in range(len(path))]\n",
    "                for j in range(len(sent_data)):\n",
    "                    fp.write(\"{}\\t{}\\t{}\\n\".format(j+1, orig_word[j], pred_tag[j]))\n",
    "                fp.write(\"\\n\")\n",
    "                sent_data = []\n",
    "                orig_word = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d5bd9a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Prior array:  (45,)\n",
      "Shape of Transition Matrix:  (45, 45)\n",
      "Shape of Emission Matrix:  (45, 20067)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of Prior array: \", prior.shape)\n",
    "print(\"Shape of Transition Matrix: \", tm.shape)\n",
    "print(\"Shape of Emission Matrix: \", em.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3917c624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev Set accuracy for Greedy Decoding of HMM:  94.02434581992593\n",
      "Dev Set accuracy for Viterbi Decoding of HMM:  94.94793880152996\n"
     ]
    }
   ],
   "source": [
    "with open(dev_path, \"r\") as fp:\n",
    "    dev_lines = fp.readlines()\n",
    "dev_lines = [line.rstrip().split(\"\\t\") for line in dev_lines]\n",
    "dev_lines = [x for x in dev_lines if x != ['']]\n",
    "accuracy = predict(dev_lines)\n",
    "print(\"Dev Set accuracy for Greedy Decoding of HMM: \",accuracy)\n",
    "accuracy = predict_viterbi(dev_lines)\n",
    "print(\"Dev Set accuracy for Viterbi Decoding of HMM: \",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2e919c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created greedy.out file...\n",
      "Created viterbi.out file...\n"
     ]
    }
   ],
   "source": [
    "with open(test_path, \"r\") as fp:\n",
    "    test_lines = fp.readlines()\n",
    "test_lines = [line.rstrip().split(\"\\t\") for line in test_lines]\n",
    "test_lines = [x for x in test_lines if x != ['']]\n",
    "test(test_lines)\n",
    "print(\"Created greedy.out file...\")\n",
    "test_viterbi(test_lines)\n",
    "print(\"Created viterbi.out file...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
